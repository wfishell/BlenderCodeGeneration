<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>o1 Experiment Analysis</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1 {
            text-align: center;
        }
        p, h2, li {
            text-align: left;
        }
        ul {
            padding-left: 20px;
        }
        .video-container {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 20px;
            margin-top: 20px;
        }
        .video-item {
            text-align: center;
            flex: 1;
        }
        video {
            width: 100%;
            height: auto;
        }
        img {
            width: 100%;
            height: auto;
            margin-top: 10px;
        }
    </style>
</head>
<body>
    <h1>o1 Experiment Analysis</h1>
    <p>This test of animation generation using the o1 and GPT-4 model showcased the ability of OpenAI's LLMs'
        to generate high-quality animations in three of the 4 test prompts. Furthermore, this experiment showed the ability of this 
        multimodal approach to create animations that build off of the prior instance and improve upon it. The three prompts that had 
        success were:
        <ul>
            <li><b>Create me a Python script for a Blender animation of a ball bouncing</b></li>
            <li><b>Create me a Python script for an object driving through a wall</b></li>
            <li><b>Create me a Python script for a Blender animation of planets orbiting around the Sun</b></li>
        </ul>
        while 
        <ul>
            <li><b>Create me a Python script for a Blender animation of a quilt falling onto a sphere</b></li>
        </ul>
        struggled to create any compelling animations and failed to improve on previous instances in any of 
        the three trials. 
    </p>
    <h2>Methodology</h2>
    <p>We ran the Blender Animation Generation pipeline a total of 12 times. This was evenly split across each prompt, with 
        each trial having 3 'evolutions': an evolution is one instance of generating an animation. If during an instance the
        LLM failed to successfully generate code after 4 queries of the LLM, we marked that trial as a failure. We initialized the start of each 
        trial with a <a href="https://wfishell.github.io/BlenderCodeGeneration/CodeTemplate.html" target="_blank" title="simple blender script">simple Blender script</a>
        of a cube moving with a light source and camera pointed at it. This approach was adopted to solve the cold start problem of generating animations without any prior information, 
        and helps the LLM from wasting instances on rendering useless information. To generate the Blender script, we used OpenAI's o1 model, as 
        it has shown to have a much lower failure rate than GPT-4 on the problem of generating animations using Blender. We rendered each animation at a resolution of 256x256 to speed up rendering times but
        output the file so that people can render at a higher resolution later. In order to then analyze the work
        of each successive generation, we used OpenAI's GPT-4-Turbo model and passed in a sequence of images. This model was used to compare the Nth-1 and Nth-2 instances and 
        pass on to the Nth instance the code from the instance that the LLM decided aligned more closely to the prompt. GPT-4-Turbo was also used to provide critiques on the instance 
        it deemed was the champion instance, and these critiques were passed into the prompt for the Nth instance. Due to recent restrictions placed by OpenAI, we had to limit
        the number of images in any one sequence to 3 images taken 40 frames apart. The breakdown of this is 3 images for each of the sets when comparing, and 3 images for the set that 
        is promoted when critiquing. Overall, this methodology is consistent with the approach we have been taking for the entire time with the exception of the size of the number
        of images passed through in the kinographs, which went from 10 images taken 20 frames apart down to the current level.
    </p>
    <p>The overall results were positive, with 10 of the 12 trials succeeding, a pass rate of 83%, resulting in 33 animations generated. The exception again being the "Quilt Falling" animations,
        which were unable to match the quality of animation that we had previously generated with 10 kinographs. This change did not have an effect on the quality of other animations, so further 
        experimentation is needed to conclusively say what caused the reduction in quality of the "Quilt Falling" animations. Otherwise, the change in render times, the increase in temporal motion, 
        humanistic analysis, and the types of errors that occurred all indicate that the code generated is of increasing complexity and leading to higher quality animations.
    </p>

    <h2>Elasped Time by Query and Render Time</h2>
    <div class="section">
        <img src="Combined_Render_Times_2024-11-14.png" alt="Error Plot" style="width: 100%; max-width: 100%; height: auto;">
    </div><br>
    <p>The Pipeline is spending less time querying in later the third instance compared to the first instance on average in all 
        prompts, but there is not a strong overall trend of the LLM spending different amounts of time on code generation in later instances.<br>
        <br>
        While this is true, interestingly enough o1 is spending almost double the amount of time on the first pass at generating the code and is 
        calling on the LLM a fewer number of times to generate an animation. In later instances the LLM is recieving more complex code and prompts and is able
        to process these in fewer numbers of queries.<br>
        <br>
        The average render times of succesive instances further illustrate the increasing complexity with three of the four prompts having average render times
        in the third instance that are 1.5-2 times longer then in the first instance.
    </p>

    <h2>Analyzing Motion Complexity with Lucas-Kanade Optical Flow</h2>
    <div class="video-container">
        <div class="video-item">
            <h3>Instance 1</h3>
            <video controls muted loop autoplay>
                <source src="https://wfishell.github.io/BlenderCodeGeneration/LucasKanadeInstance1_reencoded.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <img src="https://wfishell.github.io/BlenderCodeGeneration/Bouncing_Balls_Instance_1Lucas_Kanade_Flow_2024-11-14.png" alt="Instance 1 Snapshot">
        </div>
        <div class="video-item">
            <h3>Instance 2</h3>
            <video controls muted loop autoplay>
                <source src="https://wfishell.github.io/BlenderCodeGeneration/LucasKanadeInstance2_reencoded.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <img src="https://wfishell.github.io/BlenderCodeGeneration/Bouncing_Balls_Instance_2Lucas_Kanade_Flow_2024-11-14.png" alt="Instance 2 Snapshot">
        </div>
        <div class="video-item">
            <h3>Instance 3</h3>
            <video controls muted loop autoplay>
                <source src="https://wfishell.github.io/BlenderCodeGeneration/LucasKanadeInstance3_reencoded.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <img src="https://wfishell.github.io/BlenderCodeGeneration/Bouncing_Balls_Instance_3Lucas_Kanade_Flow_2024-11-14.png" alt="Instance 3 Snapshot">
        </div>
    </div>
    <p>While the visual does not showcase it, the recentering of the camera from instance 1 to instance 2, alongside with more dynamic shadwoing
        is the primary cause of improved measure in motion variance, mean motion and overall motion between the two instances<br>
        <br>
        Using Lucas-Kanade confirms what is visually apparent, that the temporal complexity of these objects increases throughout each instance. Specifically,
        the change in

       <h2>Code Generation Changes</h2>
        <p>The improvement in temporal and spatial complexity is directly due to the crtitques provided at each successive generation and can be seen in the changes in the generated
        code<br>
        <br>
        <b>GPT-4 Critque After First Instance</b>: "To enhance the realism of this kinograph showcasing bouncing balls, consider incorporating more naturalistic motion physics; currently, 
            each ball appears to hover without clear interaction with the surface. Adding shadow detail and slightly varying the bounces and rebound speeds of each ball according to their 
            apparent material and weight could add more depth and realism to the animation."<br></p>
        
<h3>Code Change Description Instance 1-2</h3>
 <div class="container">
    <img src="CodeChange_2_Instance1_to_2.png" alt="Code Change 1 Instance 1-2">
    <div class="description">
        <p>
            This code change shows the LLM altering how it uses Blender's BSDF shader to create more dynamic coloring of the balls. 
            This provides more realism to the balls and directly relates to the critique provided by the LLM. The material was updated 
            to add more depth and realism. Additionally, the new instance includes the following line of code:
        </p>
        <pre>light.data.angle = 0.5  # Soften the shadows</pre>
        <p>
            This change helps create more dynamic shadowing on the objects as they fall, addressing the LLM's critique of the first animation.
        </p>
    </div>
        <b>GPT-4 Critque After Second Instance</b>: "To better fit the prompt "Bouncing Balls," consider increasing the range and frequency of 
         the bounce movements of the balls to make them more dynamic and visually indicative of actual bouncing."<br></p>

     <h3>Code Change Description Instance 1-2</h3>
 <div class="container">
    <img src="CodeChange_1_Instance2_to_3.png" alt="Code Change 1 Instance 1-2">
    <div class="description">
        <p>
        The main changes in this code are the random initialization of the balls, as well as the use of Blender's physics engine to let the balls bounce and move
        freely. These changes in lines 28-30 and 37-40 respectively are directly from the LLM's critque of instance 2. 
        </p>
    </div>

     <h2>Error Rates Across Instances</h2>
     <p>The quality of these animations on average is increasing over each succesive instance. This is exemplified by the overall render times, increasing of motion in specific trials,
     and the specific changes made in the code. The next question is how does the LLM deal with generating increasingly complex code?</p><br>
     <br>
     <div class="container">
    <img src="ErrorEvolution_2024-11-14.png" alt="Code Change 1 Instance 1-2">
    <div class="description">
        <p>
    More trials need to be run to truly see understand the relationship between each successive instance and the frequency of errors and what types of errors appear, however it is worth noting
    that there is an increase in context errors and corresponding decrease in attribute errors from instance 1 to instance 3. There are two possibilites:<br> </p>
    <li>This is nothing and there is just a substituion effect in how we are identifying context errors and attribute errors and the same frequency of errors is happeneing across instances</li><br>
    <li>We have seen that as that code generated in each succesive iteration delves deeper into the blender package. Instead of making simple attribute errors, the code is making context errors where 
    it is reffering to libaries that have not been properly referenced. This indicates the LLM is potentially trying to pull in more tools to improve its work</li>
    <p><b>We will look into in more detail how these errors actually differ once we have gathered a larger sample</b></p>
    </div>
    <br>
    <h2>Error Rates Across Test Prompts</h2>
    <p>The types of errors are evenly spread across the different prompts, but qualitative analysis of the animaitons indicates that few errors often indicates little iteration from each successeive instance.
    This was seen in all trials of "Quilt Falling" and some of the "Planet Orbitting" trials. While counterintutive, we hypothosize that the lack of errors indicates the LLM is merely passing through the previous generation
    of code</p>
    <div class="container">
    <img src="ErrorBarGraph_2024-11-14.png" alt="Code Change 1 Instance 1-2">
    <div class="description">
