<!DOCTYPE html>
<html>
<head>
    <title>Embed Videos Side by Side with Autoplay</title>
    <style>
        /* Video Container Styles */
        .video-container {
            display: flex;
            flex-wrap: wrap;
            gap: 20px; /* Space between videos */
        }

        .video-item {
            flex: 1 1 calc(50% - 20px); /* Each video item takes up 50% minus the gap */
            box-sizing: border-box;
        }

        .video-item video {
            width: 100%; /* Video fills its container */
            height: auto; /* Maintains aspect ratio */
            display: block;
        }

        h1, h2, h3 {
            text-align: center;
        }

        /* Responsive Video Item for Smaller Screens */
        @media (max-width: 600px) {
            .video-item, .media-item {
                flex: 1 1 100%; /* Items stack vertically on small screens */
            }
        }

        /* Project Overview Section Styles */
        .section {
            margin-top: 40px;
            padding: 10px;
        }

        .section h2 {
            text-align: left;
            margin-bottom: 10px;
        }

        .section p {
            font-size: 16px;
            line-height: 1.6;
            text-align: justify;
        }

        /* Code Template Section Styles */
        .code-template {
            display: flex;
            align-items: flex-start; /* Align items at the top */
            gap: 20px; /* Space between image and text */
            margin-top: 40px;
        }

        .code-template img {
            width: 100%; /* Make image fill the container */
            max-width: 480px; /* Set max width to be similar to the video size */
            height: auto; /* Maintain aspect ratio */
            flex-shrink: 0; /* Prevent image from shrinking */
        }

        .code-template-content {
            flex: 1; /* Allow text to take remaining space */
        }

        .code-template-content h2 {
            text-align: left;
            margin-bottom: 10px;
        }

        .code-template-content p,
        .code-template-content pre {
            font-size: 16px;
            line-height: 1.6;
            text-align: justify;
        }

        /* Style for Code Block */
        pre {
            background-color: #f4f4f4; /* Light background for readability */
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto; /* Handle long lines with horizontal scrolling */
        }

        /* Media Container Styles */
        .media-container {
            display: flex;
            flex-wrap: wrap;
            gap: 20px; /* Space between items */
            margin-top: 20px;
        }

        .media-item {
            flex: 1 1 calc(50% - 20px); /* Each item takes up 50% minus the gap */
            box-sizing: border-box;
        }

        .media-item img,
        .media-item video {
            width: 100%; /* Media fills its container */
            height: auto; /* Maintains aspect ratio */
            display: block;
        }
    </style>
</head>
<body>
    <h1>Animation Generation Using Large Language Models</h1>

    <div class="video-container">
        <div class="video-item">
            <h2>Quilt Falling</h2>
            <video controls loop autoplay muted>
                <source src="QuiltFalling01Video.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>

        <div class="video-item">
            <h2>Orbits</h2>
            <video controls loop autoplay muted>
                <source src="Orbits01Video.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
    </div>

    <!-- Project Overview Section -->
    <div class="section">
        <h2>Project Overview</h2>
        <p>
            We are using large language models to generate animations in Blender by prompting the LLM to generate code 
            which is used to create the animation. We have developed an iterative pipeline that generates an animation, 
            then passes the finished product back into the LLM. The LLM critiques the animation based on how accurately 
            it represents the inputted prompt and then provides an updated prompt along with the generated code for the 
            next iteration of animation. This process offers clear insights into how LLMs can critique their own work 
            and use that information to improve. It also provides interesting results in how LLMs transition from a prompt 
            to the 3D space, showcasing their ability to handle the complexity of spatial and temporal reasoning in 3D environments.
        </p>
    </div>

    <!-- Code Template Section -->
    <div class="code-template-content">
        <h2>Initial Code Template</h2>
        <p>
            This initial code template that is fed into the LLM creates an object, camera, and light source. 
            This template helps the LLM in initializing code during the first iteration. Having the first iteration 
            generate a proper animation ensures that successive iterations have a solid foundation to build upon. 
        </p>
        <pre>
import bpy

# Clear existing mesh objects
bpy.ops.object.select_all(action='DESELECT')
bpy.ops.object.select_by_type(type='MESH')
bpy.ops.object.delete()

# Create a new cube
bpy.ops.mesh.primitive_cube_add(size=1, location=(0, 0, 0))
cube = bpy.context.object

# Add a point light
bpy.ops.object.light_add(type='POINT', location=(0, 0, 5))
light = bpy.context.object
light.data.energy = 1000  # Set the light strength

# Set the number of frames for the animation
start_frame = 1
end_frame = 200
bpy.context.scene.frame_start = start_frame
bpy.context.scene.frame_end = end_frame

# Insert keyframes for the cube's location
cube.location.x = -5  # Starting position
cube.keyframe_insert(data_path="location", frame=start_frame)

cube.location.x = 5  # Ending position
cube.keyframe_insert(data_path="location", frame=end_frame)

# Insert keyframes for the camera's location
camera = bpy.data.cameras.new("Camera")
camera_object = bpy.data.objects.new("Camera", camera)
bpy.context.scene.collection.objects.link(camera_object)
bpy.context.scene.camera = camera_object

# Set the camera's initial position
camera_object.location = (-5, -10, 5)
camera_object.rotation_euler = (1.1, 0, 0)  # Point down at the cube

# Insert keyframes for the camera's location
camera_object.keyframe_insert(data_path="location", frame=start_frame)
camera_object.location.x = 5  # Move camera with the cube
camera_object.keyframe_insert(data_path="location", frame=end_frame)

# Ensure all objects are baked for animation
bpy.context.view_layer.objects.active = cube
bpy.ops.nla.bake(frame_start=start_frame, frame_end=end_frame, 
                 visual_keying=True, clear_constraints=True, 
                 bake_types={'POSE', 'OBJECT'})

bpy.context.view_layer.objects.active = camera_object
bpy.ops.nla.bake(frame_start=start_frame, frame_end=end_frame, 
                 visual_keying=True, clear_constraints=True, 
                 bake_types={'POSE', 'OBJECT'})

# Set render output settings

bpy.context.scene.render.image_settings.file_format = 'FFMPEG'
bpy.context.scene.render.ffmpeg.format = 'MPEG4'
bpy.context.scene.render.ffmpeg.codec = 'H264'
        </pre>
        <p>
            Since including this template code, we have seen an improvement in the quality of the generated animations. 
            These animations are more closely aligned with the prompt, have a lower number of errors, 
            and enable us to have a standard starting point for each animation, making the results more replicable.
        </p>
    </div>

    <!-- Kinographic Feedback Loop Section -->
    <div class="code-template-content">
        <h2>Kinographic Feedback Loop</h2>
        <p>
            Many LLMs are not able to take in an mp4 video as an input, so we have created a feedback loop that inputs a kinograph. This 
            enables the LLM to critique its own work from each previous iteration. This loop has three steps:
        </p>
        <ol>
            <li>Take the previous iteration's animation and convert it into a kinograph.</li>
            <li>Input the previous animation's kinograph, the penultimate iteration's kinograph, and the original prompt into the LLM and compare which 
                animation better matches the original prompt.</li> 
            <li>Take the better performing animation and input it back into the LLM for feedback. This feedback is used to inform the next iteration's prompt.</li>
        </ol>
        <h3>Kinograph Conversion</h3>

        <!-- New Media Content -->
        
            <div class="media-item">
                <video controls loop autoplay muted>
                    <source src="PlanetaryOrbitAnimation_o1.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
        <div class="media-container">
            <div class="media-item">
                <img src="OrbitsKinographFinal.jpg" alt="Kinograph Image">
            </div>
        </div>
        <p>
        The Animation is cut at every 20 frames to create a sequence of images which is fed into the LLM
        </p>
    </div>
</body>
</html>
